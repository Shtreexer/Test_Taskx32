{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import RobertaTokenizerFast, DataCollatorForTokenClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_EPOCH = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 983k/983k [00:00<00:00, 3.59MB/s]\n",
      "Generating train split: 100%|██████████| 14041/14041 [00:02<00:00, 6093.76 examples/s]\n",
      "Generating validation split: 100%|██████████| 3250/3250 [00:00<00:00, 5638.57 examples/s]\n",
      "Generating test split: 100%|██████████| 3453/3453 [00:00<00:00, 6607.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('conll2003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'roberta-base'\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_id, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "\n",
    "  new_labels =[]\n",
    "  current_word = None\n",
    "\n",
    "  for word_id in word_ids:\n",
    "    if word_id != current_word:\n",
    "      current_word = word_id\n",
    "      label = -100 if word_id is None else labels[word_id]\n",
    "      new_labels.append(label)\n",
    "\n",
    "    elif word_id is None:\n",
    "      new_labels.append(-100)\n",
    "\n",
    "    else:\n",
    "      label = labels[word_id]\n",
    "\n",
    "      if label % 2 == 1:\n",
    "        label +=1\n",
    "\n",
    "      new_labels.append(label)\n",
    "\n",
    "  return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_function(dataset):\n",
    "  out = tokenizer(dataset['tokens'], truncation = True, is_split_into_words = True)\n",
    "  out['labels'] = align_labels_with_tokens(dataset['ner_tags'], out.word_ids())\n",
    "\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 14041/14041 [00:09<00:00, 1529.69 examples/s]\n",
      "Map: 100%|██████████| 3250/3250 [00:02<00:00, 1503.03 examples/s]\n",
      "Map: 100%|██████████| 3453/3453 [00:01<00:00, 1850.59 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenizer_function, remove_columns = ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer = tokenizer,\n",
    "                                                   return_tensors = 'tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = tokenized_dataset['train'].to_tf_dataset(\n",
    "    collate_fn = data_collator,\n",
    "    shuffle = True,\n",
    "    batch_size = BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.experimental.save(dataset=tf_train_dataset, path='Dataset/Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_val_dataset = tokenized_dataset['validation'].to_tf_dataset(\n",
    "    collate_fn = data_collator,\n",
    "    shuffle = True,\n",
    "    batch_size = BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.experimental.save(dataset=tf_val_dataset, path='Dataset/Valid')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Nearal_Workspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
